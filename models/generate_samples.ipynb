{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "from PIL import Image\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "\n",
    "# Setup\n",
    "diffusion_model_id = 'runwayml/stable-diffusion-v1-5'\n",
    "text_encoder_model_id = 'openai/clip-vit-large-patch14'\n",
    "device = 'cuda'\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Textual inversion settings\n",
    "property_name = 'deku_tree'  # Name of learned property\n",
    "placeholder_token = f'<{property_name.replace(\"_\", \"-\")}>'  # Token that represents new property\n",
    "model_path = f'saved_models/textual_inversion/{property_name}/{property_name}_best.pt'\n",
    "generated_images_path = f'generated_images/samples/{property_name}'\n",
    "if not os.path.isdir(generated_images_path):\n",
    "    os.makedirs(generated_images_path)\n",
    "\n",
    "# Hugging Face access token\n",
    "token = ''\n",
    "with open('hugging_face_token.txt', 'r') as secret:\n",
    "    token = secret.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model components\n",
    "\n",
    "# Text Encoder + Tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(text_encoder_model_id)\n",
    "text_encoder = CLIPTextModel.from_pretrained(text_encoder_model_id, torch_dtype=torch.float16)\n",
    "\n",
    "# Variational Autoencoder\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    diffusion_model_id, subfolder='vae', torch_dtype=torch.float16,\n",
    "    revision='fp16', use_auth_token=token)\n",
    "vae.to(device)\n",
    "\n",
    "# U-Net Model\n",
    "u_net = UNet2DConditionModel.from_pretrained(\n",
    "    diffusion_model_id, subfolder='unet', torch_dtype=torch.float16,\n",
    "    revision='fp16', use_auth_token=token)\n",
    "u_net.to(device)\n",
    "\n",
    "# Noise Scheduler\n",
    "scheduler = PNDMScheduler.from_config(diffusion_model_id, subfolder='scheduler', use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup tokenizer and text encoder\n",
    "\n",
    "# Add the placeholder token in tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
    "\n",
    "# Convert the placeholder token to ids\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "\n",
    "# Resize the token embeddings as we are adding new special tokens to the tokenizer\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Initialize the newly added placeholder token embedding\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "\n",
    "# Load trained embeddings\n",
    "token_embeds[placeholder_token_id] = torch.load(model_path)[placeholder_token]\n",
    "\n",
    "# Send text encoder to GPU\n",
    "text_encoder.to(device)\n",
    "print('Loaded model successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "\n",
    "# Encode input prompt\n",
    "def encode_prompt(prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt, padding='max_length', max_length=tokenizer.model_max_length,\n",
    "        truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_inputs.input_ids.to(device))[0]\n",
    "    return text_embeddings\n",
    "\n",
    "# Prepare latents for denoising\n",
    "def prepare_latents(height=512, width=512, latents=None):\n",
    "    if latents is None:\n",
    "        shape = (1, u_net.in_channels, height // 8, width // 8)\n",
    "        latents = torch.randn(shape, device=device, dtype=torch.float16)\n",
    "    latents = latents.to(device)\n",
    "    latents *= scheduler.init_noise_sigma\n",
    "    return latents\n",
    "\n",
    "# Decode latents into an image\n",
    "def decode_latents(latents):\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def eval_model(prompt, num_inference_steps=50, guidance_scale=7.5, seed=None):\n",
    "    # Set random seed\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Encode input prompt\n",
    "    prompt_embeddings = encode_prompt(prompt)\n",
    "\n",
    "    # Add unconditional embeddings for guidance scale\n",
    "    unconditional_input = tokenizer(\n",
    "        [''], padding='max_length', max_length=tokenizer.model_max_length,\n",
    "        truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        unconditional_embeddings = text_encoder(unconditional_input.input_ids.to(device))[0]\n",
    "\n",
    "    # Concatenate for final embeddings\n",
    "    text_embeddings = torch.cat([unconditional_embeddings, prompt_embeddings])\n",
    "\n",
    "    # Prepare timesteps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = scheduler.timesteps\n",
    "\n",
    "    # Prepare latent variables\n",
    "    latents = prepare_latents()\n",
    "\n",
    "    # Denoising Loop\n",
    "    for i, t in enumerate(timesteps):\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = u_net(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # Perform guidance\n",
    "        noise_pred_unconditional, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_unconditional + guidance_scale * (noise_pred_text - noise_pred_unconditional)\n",
    "\n",
    "        # Compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # Decode latents\n",
    "    decoded = decode_latents(latents)\n",
    "\n",
    "    # Return image\n",
    "    images = (decoded * 255).round().astype('uint8')\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tree with a face in the forest\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "prompt = f'A {placeholder_token} in the forest'\n",
    "#vanilla_property = 'tree'\n",
    "#prompt = f'A {vanilla_property} with a face in the forest'\n",
    "#prompt = f'A photo of {vanilla_property} on the moon'\n",
    "#prompt = f'An oil painting of {vanilla_property}'\n",
    "#prompt = f'Elmo holding a {vanilla_property}'\n",
    "#prompt = 'A photo of a dragon'\n",
    "print(prompt)\n",
    "seed = None\n",
    "guidance_scale = 7.5\n",
    "n_samples = 10\n",
    "\n",
    "# Final image\n",
    "for n in range(n_samples):\n",
    "    generated_img = eval_model(prompt, seed=seed, guidance_scale=guidance_scale)\n",
    "    generated_img.save(f'{generated_images_path}/{n + 1}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cc8010bf3f78e9c10d4febe712c77abe75f8df416c09ebdb6a9b39023fa5c08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
