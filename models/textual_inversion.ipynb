{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from itertools import chain\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import RandomHorizontalFlip\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# Setup\n",
    "diffusion_model_id = 'runwayml/stable-diffusion-v1-5'\n",
    "text_encoder_model_id = 'openai/clip-vit-large-patch14'\n",
    "device = 'cuda'\n",
    "seed = 1024\n",
    "\n",
    "# Hugging Face access token\n",
    "token = ''\n",
    "with open('hugging_face_token.txt', 'r') as secret:\n",
    "    token = secret.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'text_projection.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model components\n",
    "\n",
    "# Text Encoder + Tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(text_encoder_model_id)\n",
    "text_encoder = CLIPTextModel.from_pretrained(text_encoder_model_id, torch_dtype=torch.float16)\n",
    "text_encoder.to(device)\n",
    "\n",
    "# Variational Autoencoder\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    diffusion_model_id, subfolder='vae', torch_dtype=torch.float16,\n",
    "    revision='fp16', use_auth_token=token)\n",
    "vae.to(device)\n",
    "\n",
    "# U-Net Model\n",
    "u_net = UNet2DConditionModel.from_pretrained(\n",
    "    diffusion_model_id, subfolder='unet', torch_dtype=torch.float16,\n",
    "    revision='fp16', use_auth_token=token)\n",
    "u_net.to(device)\n",
    "\n",
    "# Noise Scheduler\n",
    "noise_scheduler = DDPMScheduler.from_config(diffusion_model_id, subfolder='scheduler', use_auth_token=token)\n",
    "\n",
    "# Freeze parameters for a model\n",
    "def freeze_params(params):\n",
    "    for param in params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Freeze all pre-trained models except for token embeddings in the text encoder\n",
    "freeze_params(vae.parameters())\n",
    "freeze_params(u_net.parameters())\n",
    "encoder_params_to_freeze = itertools.chain(\n",
    "        text_encoder.text_model.encoder.parameters(),\n",
    "        text_encoder.text_model.final_layer_norm.parameters(),\n",
    "        text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "freeze_params(encoder_params_to_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tokenizer and text encoder\n",
    "\n",
    "# Special tokens\n",
    "initializer_token = 'toy'  # Initial embedding for new property\n",
    "placeholder_token = '<object>'  # Token that represents new property\n",
    "\n",
    "# Add the placeholder token in tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
    "\n",
    "# Convert the initializer_token, placeholder_token to ids\n",
    "token_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n",
    "initializer_token_id = token_ids[0]\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "\n",
    "# Resize the token embeddings as we are adding new special tokens to the tokenizer\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Initialise the newly added placeholder token with the embeddings of the initializer token\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "object_templates = [\n",
    "    'a photo of a {}',\n",
    "    'a rendering of a {}',\n",
    "    'a cropped photo of the {}',\n",
    "    'the photo of a {}',\n",
    "    'a photo of a clean {}',\n",
    "    'a photo of a dirty {}',\n",
    "    'a dark photo of the {}',\n",
    "    'a photo of my {}',\n",
    "    'a photo of the cool {}',\n",
    "    'a close-up photo of a {}',\n",
    "    'a bright photo of the {}',\n",
    "    'a cropped photo of a {}',\n",
    "    'a photo of the {}',\n",
    "    'a good photo of the {}',\n",
    "    'a photo of one {}',\n",
    "    'a close-up photo of the {}',\n",
    "    'a rendition of the {}',\n",
    "    'a photo of the clean {}',\n",
    "    'a rendition of a {}',\n",
    "    'a photo of a nice {}',\n",
    "    'a good photo of a {}',\n",
    "    'a photo of the nice {}',\n",
    "    'a photo of the small {}',\n",
    "    'a photo of the weird {}',\n",
    "    'a photo of the large {}',\n",
    "    'a photo of a cool {}',\n",
    "    'a photo of a small {}',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TextualInversionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        learnable_property='object',\n",
    "        repeats=10,  # 100\n",
    "        flip_p=0.5,\n",
    "        set='train',\n",
    "        placeholder_token='<object>',\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.learnable_property = learnable_property\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.flip_p = flip_p\n",
    "        self.flip_transform = RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "        # Data settings\n",
    "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
    "        self.num_images = len(self.image_paths)\n",
    "        self._length = self.num_images\n",
    "        if set == 'train':\n",
    "            self._length = self.num_images * repeats\n",
    "        self.templates = object_templates\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Get and prepare image\n",
    "        image = Image.open(self.image_paths[i % self.num_images])\n",
    "        image = self.flip_transform(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float16)\n",
    "\n",
    "        # Get text prompt\n",
    "        text = random.choice(self.templates).format(self.placeholder_token)\n",
    "\n",
    "        # Create example\n",
    "        example = {}\n",
    "        example['input_prompt'] = text\n",
    "        example['pixel_values'] = torch.from_numpy(image).permute(2, 0, 1).to(device)\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "\n",
    "# Encode input prompt\n",
    "def encode_prompt(prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt, padding='max_length', max_length=tokenizer.model_max_length,\n",
    "        truncation=True, return_tensors='pt')\n",
    "    text_embeddings = text_encoder(text_inputs.input_ids.to(device))[0]\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to a pytorch file\n",
    "def save_model(path_dir, filename):\n",
    "    if not os.path.isdir(path_dir):\n",
    "        os.makedirs(path_dir)\n",
    "    learned_embeddings = text_encoder.get_input_embeddings().weight[placeholder_token_id]\n",
    "    torch.save({placeholder_token: learned_embeddings.detach().cpu()}, os.path.join(path_dir, filename))\n",
    "\n",
    "# Model training\n",
    "def train_model(property, data_root, optimizer, num_train_epochs=50, batch_size=1):\n",
    "    # Initialize dataset\n",
    "    train_dataset = TextualInversionDataset(data_root)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    max_train_steps = num_train_epochs * len(train_dataloader)\n",
    "    lr_scheduler = LambdaLR(optimizer, lambda _: 1)\n",
    "\n",
    "    # Training loop\n",
    "    print('***** Running Training *****')\n",
    "    print(f'  Num. Examples = {len(train_dataset)}')\n",
    "    print(f'  Num. Epochs = {num_train_epochs}')\n",
    "    loss_queue = deque(maxlen=10)\n",
    "    for epoch in range(num_train_epochs):\n",
    "        # Save current model\n",
    "        if not (epoch % 10):\n",
    "            save_model(f'saved_models/{property}', f'{property}_{epoch // 10}.pt')\n",
    "\n",
    "        # Train for another epoch\n",
    "        text_encoder.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Convert images to latent space\n",
    "            latents = vae.encode(batch['pixel_values']).latent_dist.sample()\n",
    "            latents *= 0.18215\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn(latents.shape, dtype=torch.float16).to(latents.device)\n",
    "            bsz = latents.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\n",
    "            ).long()\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = encode_prompt(batch['input_prompt'])\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = u_net(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "            # Backwards pass\n",
    "            loss = F.mse_loss(noise_pred, noise, reduction='none').mean([1, 2, 3]).mean()\n",
    "            loss.backward()\n",
    "\n",
    "            # Zero out the gradients for all token embeddings except the placeholder token\n",
    "            grads = text_encoder.get_input_embeddings().weight.grad\n",
    "            index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "            grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
    "\n",
    "            # Optimizer pass\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Print logs\n",
    "            current_loss = round(loss.detach().item(), 4)\n",
    "            loss_queue.append(current_loss)\n",
    "            recent_loss = round(sum(loss_queue) / 10, 4)\n",
    "            print(f'loss: {current_loss}, last_10: {recent_loss}, lr: {lr_scheduler.get_last_lr()[0]}, '\n",
    "                  f'epoch: {epoch + 1}, step: {step + 1}/{len(train_dataloader)}')\n",
    "    print('***** Training Completed *****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running Training *****\n",
      "  Num. Examples = 60\n",
      "  Num. Epochs = 5\n",
      "loss: 0.044, last_10: 0.0044, lr: 0.01, epoch: 1, step: 1/60\n",
      "loss: 0.3162, last_10: 0.036, lr: 0.01, epoch: 1, step: 2/60\n",
      "loss: 0.05, last_10: 0.041, lr: 0.01, epoch: 1, step: 3/60\n",
      "loss: 0.0212, last_10: 0.0431, lr: 0.01, epoch: 1, step: 4/60\n",
      "loss: 0.1116, last_10: 0.0543, lr: 0.01, epoch: 1, step: 5/60\n",
      "loss: 0.0041, last_10: 0.0547, lr: 0.01, epoch: 1, step: 6/60\n",
      "loss: 0.322, last_10: 0.0869, lr: 0.01, epoch: 1, step: 7/60\n",
      "loss: 0.3059, last_10: 0.1175, lr: 0.01, epoch: 1, step: 8/60\n",
      "loss: 0.0262, last_10: 0.1201, lr: 0.01, epoch: 1, step: 9/60\n",
      "loss: 0.01, last_10: 0.1211, lr: 0.01, epoch: 1, step: 10/60\n",
      "loss: 0.0032, last_10: 0.117, lr: 0.01, epoch: 1, step: 11/60\n",
      "loss: 0.2585, last_10: 0.1113, lr: 0.01, epoch: 1, step: 12/60\n",
      "loss: 0.0111, last_10: 0.1074, lr: 0.01, epoch: 1, step: 13/60\n",
      "loss: 0.0332, last_10: 0.1086, lr: 0.01, epoch: 1, step: 14/60\n",
      "loss: 0.186, last_10: 0.116, lr: 0.01, epoch: 1, step: 15/60\n",
      "loss: 0.0245, last_10: 0.1181, lr: 0.01, epoch: 1, step: 16/60\n",
      "loss: 0.0978, last_10: 0.0956, lr: 0.01, epoch: 1, step: 17/60\n",
      "loss: 0.3518, last_10: 0.1002, lr: 0.01, epoch: 1, step: 18/60\n",
      "loss: 0.332, last_10: 0.1308, lr: 0.01, epoch: 1, step: 19/60\n",
      "loss: 0.0583, last_10: 0.1356, lr: 0.01, epoch: 1, step: 20/60\n",
      "loss: 0.0116, last_10: 0.1365, lr: 0.01, epoch: 1, step: 21/60\n",
      "loss: 0.1001, last_10: 0.1206, lr: 0.01, epoch: 1, step: 22/60\n",
      "loss: 0.0067, last_10: 0.1202, lr: 0.01, epoch: 1, step: 23/60\n",
      "loss: 0.158, last_10: 0.1327, lr: 0.01, epoch: 1, step: 24/60\n",
      "loss: 0.0089, last_10: 0.115, lr: 0.01, epoch: 1, step: 25/60\n",
      "loss: 0.4822, last_10: 0.1607, lr: 0.01, epoch: 1, step: 26/60\n",
      "loss: 0.0256, last_10: 0.1535, lr: 0.01, epoch: 1, step: 27/60\n",
      "loss: 0.147, last_10: 0.133, lr: 0.01, epoch: 1, step: 28/60\n",
      "loss: 0.1138, last_10: 0.1112, lr: 0.01, epoch: 1, step: 29/60\n",
      "loss: 0.1158, last_10: 0.117, lr: 0.01, epoch: 1, step: 30/60\n",
      "loss: 0.0913, last_10: 0.1249, lr: 0.01, epoch: 1, step: 31/60\n",
      "loss: 0.0925, last_10: 0.1242, lr: 0.01, epoch: 1, step: 32/60\n",
      "loss: 0.2244, last_10: 0.1459, lr: 0.01, epoch: 1, step: 33/60\n",
      "loss: 0.0456, last_10: 0.1347, lr: 0.01, epoch: 1, step: 34/60\n",
      "loss: 0.0072, last_10: 0.1345, lr: 0.01, epoch: 1, step: 35/60\n",
      "loss: 0.0142, last_10: 0.0877, lr: 0.01, epoch: 1, step: 36/60\n",
      "loss: 0.1897, last_10: 0.1041, lr: 0.01, epoch: 1, step: 37/60\n",
      "loss: 0.0942, last_10: 0.0989, lr: 0.01, epoch: 1, step: 38/60\n",
      "loss: 0.1298, last_10: 0.1005, lr: 0.01, epoch: 1, step: 39/60\n",
      "loss: 0.1127, last_10: 0.1002, lr: 0.01, epoch: 1, step: 40/60\n",
      "loss: 0.384, last_10: 0.1294, lr: 0.01, epoch: 1, step: 41/60\n",
      "loss: 0.3838, last_10: 0.1586, lr: 0.01, epoch: 1, step: 42/60\n",
      "loss: 0.1566, last_10: 0.1518, lr: 0.01, epoch: 1, step: 43/60\n",
      "loss: 0.1743, last_10: 0.1647, lr: 0.01, epoch: 1, step: 44/60\n",
      "loss: 0.1475, last_10: 0.1787, lr: 0.01, epoch: 1, step: 45/60\n",
      "loss: 0.3796, last_10: 0.2152, lr: 0.01, epoch: 1, step: 46/60\n",
      "loss: 0.1802, last_10: 0.2143, lr: 0.01, epoch: 1, step: 47/60\n",
      "loss: 0.0554, last_10: 0.2104, lr: 0.01, epoch: 1, step: 48/60\n",
      "loss: 0.0151, last_10: 0.1989, lr: 0.01, epoch: 1, step: 49/60\n",
      "loss: 0.0034, last_10: 0.188, lr: 0.01, epoch: 1, step: 50/60\n",
      "loss: 0.6475, last_10: 0.2143, lr: 0.01, epoch: 1, step: 51/60\n",
      "loss: 0.0125, last_10: 0.1772, lr: 0.01, epoch: 1, step: 52/60\n",
      "loss: 0.2306, last_10: 0.1846, lr: 0.01, epoch: 1, step: 53/60\n",
      "loss: 0.1879, last_10: 0.186, lr: 0.01, epoch: 1, step: 54/60\n",
      "loss: 0.4177, last_10: 0.213, lr: 0.01, epoch: 1, step: 55/60\n",
      "loss: 0.0144, last_10: 0.1765, lr: 0.01, epoch: 1, step: 56/60\n",
      "loss: 0.0536, last_10: 0.1638, lr: 0.01, epoch: 1, step: 57/60\n",
      "loss: 0.1609, last_10: 0.1744, lr: 0.01, epoch: 1, step: 58/60\n",
      "loss: 0.1538, last_10: 0.1882, lr: 0.01, epoch: 1, step: 59/60\n",
      "loss: 0.0905, last_10: 0.1969, lr: 0.01, epoch: 1, step: 60/60\n",
      "loss: 0.2595, last_10: 0.1581, lr: 0.01, epoch: 2, step: 1/60\n",
      "loss: 0.0895, last_10: 0.1658, lr: 0.01, epoch: 2, step: 2/60\n",
      "loss: 0.1335, last_10: 0.1561, lr: 0.01, epoch: 2, step: 3/60\n",
      "loss: 0.0955, last_10: 0.1469, lr: 0.01, epoch: 2, step: 4/60\n",
      "loss: 0.5054, last_10: 0.1557, lr: 0.01, epoch: 2, step: 5/60\n",
      "loss: 0.0542, last_10: 0.1596, lr: 0.01, epoch: 2, step: 6/60\n",
      "loss: 0.3308, last_10: 0.1874, lr: 0.01, epoch: 2, step: 7/60\n",
      "loss: 0.0182, last_10: 0.1731, lr: 0.01, epoch: 2, step: 8/60\n",
      "loss: 0.1653, last_10: 0.1742, lr: 0.01, epoch: 2, step: 9/60\n",
      "loss: 0.2664, last_10: 0.1918, lr: 0.01, epoch: 2, step: 10/60\n",
      "loss: 0.0063, last_10: 0.1665, lr: 0.01, epoch: 2, step: 11/60\n",
      "loss: 0.0037, last_10: 0.1579, lr: 0.01, epoch: 2, step: 12/60\n",
      "loss: 0.311, last_10: 0.1757, lr: 0.01, epoch: 2, step: 13/60\n",
      "loss: 0.2825, last_10: 0.1944, lr: 0.01, epoch: 2, step: 14/60\n",
      "loss: 0.2605, last_10: 0.1699, lr: 0.01, epoch: 2, step: 15/60\n",
      "loss: 0.0602, last_10: 0.1705, lr: 0.01, epoch: 2, step: 16/60\n",
      "loss: 0.0055, last_10: 0.138, lr: 0.01, epoch: 2, step: 17/60\n",
      "loss: 0.2219, last_10: 0.1583, lr: 0.01, epoch: 2, step: 18/60\n",
      "loss: 0.069, last_10: 0.1487, lr: 0.01, epoch: 2, step: 19/60\n",
      "loss: 0.0047, last_10: 0.1225, lr: 0.01, epoch: 2, step: 20/60\n",
      "loss: 0.0967, last_10: 0.1316, lr: 0.01, epoch: 2, step: 21/60\n",
      "loss: 0.0773, last_10: 0.1389, lr: 0.01, epoch: 2, step: 22/60\n",
      "loss: 0.0313, last_10: 0.111, lr: 0.01, epoch: 2, step: 23/60\n",
      "loss: 0.0262, last_10: 0.0853, lr: 0.01, epoch: 2, step: 24/60\n",
      "loss: 0.1285, last_10: 0.0721, lr: 0.01, epoch: 2, step: 25/60\n",
      "loss: 0.1364, last_10: 0.0798, lr: 0.01, epoch: 2, step: 26/60\n",
      "loss: 0.0032, last_10: 0.0795, lr: 0.01, epoch: 2, step: 27/60\n",
      "loss: 0.0158, last_10: 0.0589, lr: 0.01, epoch: 2, step: 28/60\n",
      "loss: 0.2957, last_10: 0.0816, lr: 0.01, epoch: 2, step: 29/60\n",
      "loss: 0.0034, last_10: 0.0814, lr: 0.01, epoch: 2, step: 30/60\n",
      "loss: 0.0032, last_10: 0.0721, lr: 0.01, epoch: 2, step: 31/60\n",
      "loss: 0.2079, last_10: 0.0852, lr: 0.01, epoch: 2, step: 32/60\n",
      "loss: 0.1832, last_10: 0.1003, lr: 0.01, epoch: 2, step: 33/60\n",
      "loss: 0.646, last_10: 0.1623, lr: 0.01, epoch: 2, step: 34/60\n",
      "loss: 0.0052, last_10: 0.15, lr: 0.01, epoch: 2, step: 35/60\n",
      "loss: 0.2615, last_10: 0.1625, lr: 0.01, epoch: 2, step: 36/60\n",
      "loss: 0.2281, last_10: 0.185, lr: 0.01, epoch: 2, step: 37/60\n",
      "loss: 0.0774, last_10: 0.1912, lr: 0.01, epoch: 2, step: 38/60\n",
      "loss: 0.2642, last_10: 0.188, lr: 0.01, epoch: 2, step: 39/60\n",
      "loss: 0.0042, last_10: 0.1881, lr: 0.01, epoch: 2, step: 40/60\n",
      "loss: 0.0121, last_10: 0.189, lr: 0.01, epoch: 2, step: 41/60\n",
      "loss: 0.5415, last_10: 0.2223, lr: 0.01, epoch: 2, step: 42/60\n",
      "loss: 0.3789, last_10: 0.2419, lr: 0.01, epoch: 2, step: 43/60\n",
      "loss: 0.3247, last_10: 0.2098, lr: 0.01, epoch: 2, step: 44/60\n",
      "loss: 0.3252, last_10: 0.2418, lr: 0.01, epoch: 2, step: 45/60\n",
      "loss: 0.0258, last_10: 0.2182, lr: 0.01, epoch: 2, step: 46/60\n",
      "loss: 0.2358, last_10: 0.219, lr: 0.01, epoch: 2, step: 47/60\n",
      "loss: 0.0884, last_10: 0.2201, lr: 0.01, epoch: 2, step: 48/60\n",
      "loss: 0.0377, last_10: 0.1974, lr: 0.01, epoch: 2, step: 49/60\n",
      "loss: 0.3357, last_10: 0.2306, lr: 0.01, epoch: 2, step: 50/60\n",
      "loss: 0.0807, last_10: 0.2374, lr: 0.01, epoch: 2, step: 51/60\n",
      "loss: 0.01, last_10: 0.1843, lr: 0.01, epoch: 2, step: 52/60\n",
      "loss: 0.0104, last_10: 0.1474, lr: 0.01, epoch: 2, step: 53/60\n",
      "loss: 0.155, last_10: 0.1305, lr: 0.01, epoch: 2, step: 54/60\n",
      "loss: 0.1411, last_10: 0.1121, lr: 0.01, epoch: 2, step: 55/60\n",
      "loss: 0.1257, last_10: 0.122, lr: 0.01, epoch: 2, step: 56/60\n",
      "loss: 0.0082, last_10: 0.0993, lr: 0.01, epoch: 2, step: 57/60\n",
      "loss: 0.0528, last_10: 0.0957, lr: 0.01, epoch: 2, step: 58/60\n",
      "loss: 0.0788, last_10: 0.0998, lr: 0.01, epoch: 2, step: 59/60\n",
      "loss: 0.4067, last_10: 0.1069, lr: 0.01, epoch: 2, step: 60/60\n",
      "loss: 0.0923, last_10: 0.1081, lr: 0.01, epoch: 3, step: 1/60\n",
      "loss: 0.0167, last_10: 0.1088, lr: 0.01, epoch: 3, step: 2/60\n",
      "loss: 0.163, last_10: 0.124, lr: 0.01, epoch: 3, step: 3/60\n",
      "loss: 0.2076, last_10: 0.1293, lr: 0.01, epoch: 3, step: 4/60\n",
      "loss: 0.0204, last_10: 0.1172, lr: 0.01, epoch: 3, step: 5/60\n",
      "loss: 0.7329, last_10: 0.1779, lr: 0.01, epoch: 3, step: 6/60\n",
      "loss: 0.055, last_10: 0.1826, lr: 0.01, epoch: 3, step: 7/60\n",
      "loss: 0.3157, last_10: 0.2089, lr: 0.01, epoch: 3, step: 8/60\n",
      "loss: 0.0172, last_10: 0.2027, lr: 0.01, epoch: 3, step: 9/60\n",
      "loss: 0.0383, last_10: 0.1659, lr: 0.01, epoch: 3, step: 10/60\n",
      "loss: 0.5464, last_10: 0.2113, lr: 0.01, epoch: 3, step: 11/60\n",
      "loss: 0.0131, last_10: 0.211, lr: 0.01, epoch: 3, step: 12/60\n",
      "loss: 0.0054, last_10: 0.1952, lr: 0.01, epoch: 3, step: 13/60\n",
      "loss: 0.1025, last_10: 0.1847, lr: 0.01, epoch: 3, step: 14/60\n",
      "loss: 0.0325, last_10: 0.1859, lr: 0.01, epoch: 3, step: 15/60\n",
      "loss: 0.1849, last_10: 0.1311, lr: 0.01, epoch: 3, step: 16/60\n",
      "loss: 0.0148, last_10: 0.1271, lr: 0.01, epoch: 3, step: 17/60\n",
      "loss: 0.0473, last_10: 0.1002, lr: 0.01, epoch: 3, step: 18/60\n",
      "loss: 0.0083, last_10: 0.0994, lr: 0.01, epoch: 3, step: 19/60\n",
      "loss: 0.2477, last_10: 0.1203, lr: 0.01, epoch: 3, step: 20/60\n",
      "loss: 0.0195, last_10: 0.0676, lr: 0.01, epoch: 3, step: 21/60\n",
      "loss: 0.3079, last_10: 0.0971, lr: 0.01, epoch: 3, step: 22/60\n",
      "loss: 0.0792, last_10: 0.1045, lr: 0.01, epoch: 3, step: 23/60\n",
      "loss: 0.0595, last_10: 0.1002, lr: 0.01, epoch: 3, step: 24/60\n",
      "loss: 0.1115, last_10: 0.1081, lr: 0.01, epoch: 3, step: 25/60\n",
      "loss: 0.0103, last_10: 0.0906, lr: 0.01, epoch: 3, step: 26/60\n",
      "loss: 0.3376, last_10: 0.1229, lr: 0.01, epoch: 3, step: 27/60\n",
      "loss: 0.0066, last_10: 0.1188, lr: 0.01, epoch: 3, step: 28/60\n",
      "loss: 0.2355, last_10: 0.1415, lr: 0.01, epoch: 3, step: 29/60\n",
      "loss: 0.0404, last_10: 0.1208, lr: 0.01, epoch: 3, step: 30/60\n",
      "loss: 0.3503, last_10: 0.1539, lr: 0.01, epoch: 3, step: 31/60\n",
      "loss: 0.0066, last_10: 0.1237, lr: 0.01, epoch: 3, step: 32/60\n",
      "loss: 0.0127, last_10: 0.1171, lr: 0.01, epoch: 3, step: 33/60\n",
      "loss: 0.019, last_10: 0.113, lr: 0.01, epoch: 3, step: 34/60\n",
      "loss: 0.0483, last_10: 0.1067, lr: 0.01, epoch: 3, step: 35/60\n",
      "loss: 0.2345, last_10: 0.1291, lr: 0.01, epoch: 3, step: 36/60\n",
      "loss: 0.2629, last_10: 0.1217, lr: 0.01, epoch: 3, step: 37/60\n",
      "loss: 0.0102, last_10: 0.122, lr: 0.01, epoch: 3, step: 38/60\n",
      "loss: 0.1426, last_10: 0.1127, lr: 0.01, epoch: 3, step: 39/60\n",
      "loss: 0.0118, last_10: 0.1099, lr: 0.01, epoch: 3, step: 40/60\n",
      "loss: 0.5464, last_10: 0.1295, lr: 0.01, epoch: 3, step: 41/60\n",
      "loss: 0.0627, last_10: 0.1351, lr: 0.01, epoch: 3, step: 42/60\n",
      "loss: 0.3572, last_10: 0.1696, lr: 0.01, epoch: 3, step: 43/60\n",
      "loss: 0.1494, last_10: 0.1826, lr: 0.01, epoch: 3, step: 44/60\n",
      "loss: 0.0969, last_10: 0.1875, lr: 0.01, epoch: 3, step: 45/60\n",
      "loss: 0.1735, last_10: 0.1814, lr: 0.01, epoch: 3, step: 46/60\n",
      "loss: 0.0152, last_10: 0.1566, lr: 0.01, epoch: 3, step: 47/60\n",
      "loss: 0.0174, last_10: 0.1573, lr: 0.01, epoch: 3, step: 48/60\n",
      "loss: 0.3242, last_10: 0.1755, lr: 0.01, epoch: 3, step: 49/60\n",
      "loss: 0.0049, last_10: 0.1748, lr: 0.01, epoch: 3, step: 50/60\n",
      "loss: 0.0513, last_10: 0.1253, lr: 0.01, epoch: 3, step: 51/60\n",
      "loss: 0.223, last_10: 0.1413, lr: 0.01, epoch: 3, step: 52/60\n",
      "loss: 0.0092, last_10: 0.1065, lr: 0.01, epoch: 3, step: 53/60\n",
      "loss: 0.0756, last_10: 0.0991, lr: 0.01, epoch: 3, step: 54/60\n",
      "loss: 0.0212, last_10: 0.0915, lr: 0.01, epoch: 3, step: 55/60\n",
      "loss: 0.0183, last_10: 0.076, lr: 0.01, epoch: 3, step: 56/60\n",
      "loss: 0.1335, last_10: 0.0879, lr: 0.01, epoch: 3, step: 57/60\n",
      "loss: 0.0578, last_10: 0.0919, lr: 0.01, epoch: 3, step: 58/60\n",
      "loss: 0.0023, last_10: 0.0597, lr: 0.01, epoch: 3, step: 59/60\n",
      "loss: 0.1427, last_10: 0.0735, lr: 0.01, epoch: 3, step: 60/60\n",
      "loss: 0.0075, last_10: 0.0691, lr: 0.01, epoch: 4, step: 1/60\n",
      "loss: 0.0194, last_10: 0.0488, lr: 0.01, epoch: 4, step: 2/60\n",
      "loss: 0.5767, last_10: 0.1055, lr: 0.01, epoch: 4, step: 3/60\n",
      "loss: 0.0915, last_10: 0.1071, lr: 0.01, epoch: 4, step: 4/60\n",
      "loss: 0.0511, last_10: 0.1101, lr: 0.01, epoch: 4, step: 5/60\n",
      "loss: 0.0208, last_10: 0.1103, lr: 0.01, epoch: 4, step: 6/60\n",
      "loss: 0.1692, last_10: 0.1139, lr: 0.01, epoch: 4, step: 7/60\n",
      "loss: 0.2922, last_10: 0.1373, lr: 0.01, epoch: 4, step: 8/60\n",
      "loss: 0.1354, last_10: 0.1507, lr: 0.01, epoch: 4, step: 9/60\n",
      "loss: 0.1287, last_10: 0.1493, lr: 0.01, epoch: 4, step: 10/60\n",
      "loss: 0.4104, last_10: 0.1895, lr: 0.01, epoch: 4, step: 11/60\n",
      "loss: 0.1731, last_10: 0.2049, lr: 0.01, epoch: 4, step: 12/60\n",
      "loss: 0.0114, last_10: 0.1484, lr: 0.01, epoch: 4, step: 13/60\n",
      "loss: 0.0037, last_10: 0.1396, lr: 0.01, epoch: 4, step: 14/60\n",
      "loss: 0.0482, last_10: 0.1393, lr: 0.01, epoch: 4, step: 15/60\n",
      "loss: 0.2089, last_10: 0.1581, lr: 0.01, epoch: 4, step: 16/60\n",
      "loss: 0.1683, last_10: 0.158, lr: 0.01, epoch: 4, step: 17/60\n",
      "loss: 0.1583, last_10: 0.1446, lr: 0.01, epoch: 4, step: 18/60\n",
      "loss: 0.0453, last_10: 0.1356, lr: 0.01, epoch: 4, step: 19/60\n",
      "loss: 0.0078, last_10: 0.1235, lr: 0.01, epoch: 4, step: 20/60\n",
      "loss: 0.0552, last_10: 0.088, lr: 0.01, epoch: 4, step: 21/60\n",
      "loss: 0.0031, last_10: 0.071, lr: 0.01, epoch: 4, step: 22/60\n",
      "loss: 0.1565, last_10: 0.0855, lr: 0.01, epoch: 4, step: 23/60\n",
      "loss: 0.1165, last_10: 0.0968, lr: 0.01, epoch: 4, step: 24/60\n",
      "loss: 0.0721, last_10: 0.0992, lr: 0.01, epoch: 4, step: 25/60\n",
      "loss: 0.088, last_10: 0.0871, lr: 0.01, epoch: 4, step: 26/60\n",
      "loss: 0.2419, last_10: 0.0945, lr: 0.01, epoch: 4, step: 27/60\n",
      "loss: 0.0066, last_10: 0.0793, lr: 0.01, epoch: 4, step: 28/60\n",
      "loss: 0.7334, last_10: 0.1481, lr: 0.01, epoch: 4, step: 29/60\n",
      "loss: 0.1321, last_10: 0.1605, lr: 0.01, epoch: 4, step: 30/60\n",
      "loss: 0.2438, last_10: 0.1794, lr: 0.01, epoch: 4, step: 31/60\n",
      "loss: 0.0101, last_10: 0.1801, lr: 0.01, epoch: 4, step: 32/60\n",
      "loss: 0.5815, last_10: 0.2226, lr: 0.01, epoch: 4, step: 33/60\n",
      "loss: 0.0188, last_10: 0.2128, lr: 0.01, epoch: 4, step: 34/60\n",
      "loss: 0.0055, last_10: 0.2062, lr: 0.01, epoch: 4, step: 35/60\n",
      "loss: 0.1572, last_10: 0.2131, lr: 0.01, epoch: 4, step: 36/60\n",
      "loss: 0.2603, last_10: 0.2149, lr: 0.01, epoch: 4, step: 37/60\n",
      "loss: 0.0026, last_10: 0.2145, lr: 0.01, epoch: 4, step: 38/60\n",
      "loss: 0.011, last_10: 0.1423, lr: 0.01, epoch: 4, step: 39/60\n",
      "loss: 0.0824, last_10: 0.1373, lr: 0.01, epoch: 4, step: 40/60\n",
      "loss: 0.023, last_10: 0.1152, lr: 0.01, epoch: 4, step: 41/60\n",
      "loss: 0.0087, last_10: 0.1151, lr: 0.01, epoch: 4, step: 42/60\n",
      "loss: 0.0534, last_10: 0.0623, lr: 0.01, epoch: 4, step: 43/60\n",
      "loss: 0.0238, last_10: 0.0628, lr: 0.01, epoch: 4, step: 44/60\n",
      "loss: 0.0066, last_10: 0.0629, lr: 0.01, epoch: 4, step: 45/60\n",
      "loss: 0.1366, last_10: 0.0608, lr: 0.01, epoch: 4, step: 46/60\n",
      "loss: 0.3875, last_10: 0.0736, lr: 0.01, epoch: 4, step: 47/60\n",
      "loss: 0.179, last_10: 0.0912, lr: 0.01, epoch: 4, step: 48/60\n",
      "loss: 0.4255, last_10: 0.1326, lr: 0.01, epoch: 4, step: 49/60\n",
      "loss: 0.0286, last_10: 0.1273, lr: 0.01, epoch: 4, step: 50/60\n",
      "loss: 0.3462, last_10: 0.1596, lr: 0.01, epoch: 4, step: 51/60\n",
      "loss: 0.0077, last_10: 0.1595, lr: 0.01, epoch: 4, step: 52/60\n",
      "loss: 0.0081, last_10: 0.155, lr: 0.01, epoch: 4, step: 53/60\n",
      "loss: 0.4971, last_10: 0.2023, lr: 0.01, epoch: 4, step: 54/60\n",
      "loss: 0.047, last_10: 0.2063, lr: 0.01, epoch: 4, step: 55/60\n",
      "loss: 0.004, last_10: 0.1931, lr: 0.01, epoch: 4, step: 56/60\n",
      "loss: 0.2222, last_10: 0.1765, lr: 0.01, epoch: 4, step: 57/60\n",
      "loss: 0.0054, last_10: 0.1592, lr: 0.01, epoch: 4, step: 58/60\n",
      "loss: 0.1328, last_10: 0.1299, lr: 0.01, epoch: 4, step: 59/60\n",
      "loss: 0.1076, last_10: 0.1378, lr: 0.01, epoch: 4, step: 60/60\n",
      "loss: 0.1512, last_10: 0.1183, lr: 0.01, epoch: 5, step: 1/60\n",
      "loss: 0.1213, last_10: 0.1297, lr: 0.01, epoch: 5, step: 2/60\n",
      "loss: 0.1493, last_10: 0.1438, lr: 0.01, epoch: 5, step: 3/60\n",
      "loss: 0.3167, last_10: 0.1257, lr: 0.01, epoch: 5, step: 4/60\n",
      "loss: 0.0236, last_10: 0.1234, lr: 0.01, epoch: 5, step: 5/60\n",
      "loss: 0.0856, last_10: 0.1316, lr: 0.01, epoch: 5, step: 6/60\n",
      "loss: 0.0374, last_10: 0.1131, lr: 0.01, epoch: 5, step: 7/60\n",
      "loss: 0.0072, last_10: 0.1133, lr: 0.01, epoch: 5, step: 8/60\n",
      "loss: 0.3982, last_10: 0.1398, lr: 0.01, epoch: 5, step: 9/60\n",
      "loss: 0.0142, last_10: 0.1305, lr: 0.01, epoch: 5, step: 10/60\n",
      "loss: 0.1691, last_10: 0.1323, lr: 0.01, epoch: 5, step: 11/60\n",
      "loss: 0.0632, last_10: 0.1264, lr: 0.01, epoch: 5, step: 12/60\n",
      "loss: 0.0076, last_10: 0.1123, lr: 0.01, epoch: 5, step: 13/60\n",
      "loss: 0.6646, last_10: 0.1471, lr: 0.01, epoch: 5, step: 14/60\n",
      "loss: 0.3567, last_10: 0.1804, lr: 0.01, epoch: 5, step: 15/60\n",
      "loss: 0.2529, last_10: 0.1971, lr: 0.01, epoch: 5, step: 16/60\n",
      "loss: 0.2439, last_10: 0.2178, lr: 0.01, epoch: 5, step: 17/60\n",
      "loss: 0.0613, last_10: 0.2232, lr: 0.01, epoch: 5, step: 18/60\n",
      "loss: 0.0321, last_10: 0.1866, lr: 0.01, epoch: 5, step: 19/60\n",
      "loss: 0.0347, last_10: 0.1886, lr: 0.01, epoch: 5, step: 20/60\n",
      "loss: 0.3337, last_10: 0.2051, lr: 0.01, epoch: 5, step: 21/60\n",
      "loss: 0.5986, last_10: 0.2586, lr: 0.01, epoch: 5, step: 22/60\n",
      "loss: 0.4219, last_10: 0.3, lr: 0.01, epoch: 5, step: 23/60\n",
      "loss: 0.0168, last_10: 0.2353, lr: 0.01, epoch: 5, step: 24/60\n",
      "loss: 0.4377, last_10: 0.2434, lr: 0.01, epoch: 5, step: 25/60\n",
      "loss: 0.0193, last_10: 0.22, lr: 0.01, epoch: 5, step: 26/60\n",
      "loss: 0.1375, last_10: 0.2094, lr: 0.01, epoch: 5, step: 27/60\n",
      "loss: 0.003, last_10: 0.2035, lr: 0.01, epoch: 5, step: 28/60\n",
      "loss: 0.1663, last_10: 0.217, lr: 0.01, epoch: 5, step: 29/60\n",
      "loss: 0.1954, last_10: 0.233, lr: 0.01, epoch: 5, step: 30/60\n",
      "loss: 0.0082, last_10: 0.2005, lr: 0.01, epoch: 5, step: 31/60\n",
      "loss: 0.1914, last_10: 0.1597, lr: 0.01, epoch: 5, step: 32/60\n",
      "loss: 0.1196, last_10: 0.1295, lr: 0.01, epoch: 5, step: 33/60\n",
      "loss: 0.2961, last_10: 0.1575, lr: 0.01, epoch: 5, step: 34/60\n",
      "loss: 0.3521, last_10: 0.1489, lr: 0.01, epoch: 5, step: 35/60\n",
      "loss: 0.0025, last_10: 0.1472, lr: 0.01, epoch: 5, step: 36/60\n",
      "loss: 0.0211, last_10: 0.1356, lr: 0.01, epoch: 5, step: 37/60\n",
      "loss: 0.0125, last_10: 0.1365, lr: 0.01, epoch: 5, step: 38/60\n",
      "loss: 0.0298, last_10: 0.1229, lr: 0.01, epoch: 5, step: 39/60\n",
      "loss: 0.0264, last_10: 0.106, lr: 0.01, epoch: 5, step: 40/60\n",
      "loss: 0.2335, last_10: 0.1285, lr: 0.01, epoch: 5, step: 41/60\n",
      "loss: 0.2988, last_10: 0.1392, lr: 0.01, epoch: 5, step: 42/60\n",
      "loss: 0.3167, last_10: 0.1589, lr: 0.01, epoch: 5, step: 43/60\n",
      "loss: 0.1063, last_10: 0.14, lr: 0.01, epoch: 5, step: 44/60\n",
      "loss: 0.0055, last_10: 0.1053, lr: 0.01, epoch: 5, step: 45/60\n",
      "loss: 0.2957, last_10: 0.1346, lr: 0.01, epoch: 5, step: 46/60\n",
      "loss: 0.1046, last_10: 0.143, lr: 0.01, epoch: 5, step: 47/60\n",
      "loss: 0.0064, last_10: 0.1424, lr: 0.01, epoch: 5, step: 48/60\n",
      "loss: 0.4854, last_10: 0.1879, lr: 0.01, epoch: 5, step: 49/60\n",
      "loss: 0.5942, last_10: 0.2447, lr: 0.01, epoch: 5, step: 50/60\n",
      "loss: 0.0072, last_10: 0.2221, lr: 0.01, epoch: 5, step: 51/60\n",
      "loss: 0.0176, last_10: 0.194, lr: 0.01, epoch: 5, step: 52/60\n",
      "loss: 0.0499, last_10: 0.1673, lr: 0.01, epoch: 5, step: 53/60\n",
      "loss: 0.0314, last_10: 0.1598, lr: 0.01, epoch: 5, step: 54/60\n",
      "loss: 0.01, last_10: 0.1602, lr: 0.01, epoch: 5, step: 55/60\n",
      "loss: 0.0683, last_10: 0.1375, lr: 0.01, epoch: 5, step: 56/60\n",
      "loss: 0.0599, last_10: 0.133, lr: 0.01, epoch: 5, step: 57/60\n",
      "loss: 0.0228, last_10: 0.1347, lr: 0.01, epoch: 5, step: 58/60\n",
      "loss: 0.0514, last_10: 0.0913, lr: 0.01, epoch: 5, step: 59/60\n",
      "loss: 0.0251, last_10: 0.0344, lr: 0.01, epoch: 5, step: 60/60\n",
      "***** Training Completed *****\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer\n",
    "# TODO: Tune all and choose best one\n",
    "#optimizer = torch.optim.SGD(text_encoder.get_input_embeddings().parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(text_encoder.get_input_embeddings().parameters(), lr=0.01, eps=1e-07)\n",
    "#optimizer = torch.optim.Adam(text_encoder.get_input_embeddings().parameters(), lr=0.001, eps=1e-08)\n",
    "#optimizer = torch.optim.Adagrad(text_encoder.get_input_embeddings().parameters(), lr=0.01, eps=1e-10)\n",
    "#optimizer = torch.optim.Adadelta(text_encoder.get_input_embeddings().parameters(), lr=1.0, eps=1e-06)\n",
    "#optimizer = torch.optim.RMSprop(text_encoder.get_input_embeddings().parameters(), lr=0.01, eps=1e-06)\n",
    "\n",
    "# Training\n",
    "property = 'cat_toy'\n",
    "data_folder = f'../data/{property}'\n",
    "train_model(property, data_folder, optimizer, num_train_epochs=5)\n",
    "\n",
    "# Save final model\n",
    "save_model(f'saved_models/{property}', f'{property}_final.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cc8010bf3f78e9c10d4febe712c77abe75f8df416c09ebdb6a9b39023fa5c08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
