{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from torch import autocast\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Setup\n",
    "#runwayml/stable-diffusion-v1-5\n",
    "diffusion_model_id = 'CompVis/stable-diffusion-v1-4'\n",
    "text_encoder_model_id = 'openai/clip-vit-large-patch14'\n",
    "device = 'cuda'\n",
    "seed_cpu = torch.Generator('cpu').manual_seed(1024)\n",
    "seed_gpu = torch.Generator('cuda').manual_seed(1024)\n",
    "\n",
    "# Hugging Face access token\n",
    "token = ''\n",
    "with open('hugging_face_token.txt', 'r') as secret:\n",
    "    token = secret.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model components\n",
    "\n",
    "# Variational Autoencoder\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    diffusion_model_id, subfolder='vae', torch_dtype=torch.float16,\n",
    "    revision='fp16', use_auth_token=token)\n",
    "vae.to(device)\n",
    "\n",
    "# U-Net Model\n",
    "u_net = UNet2DConditionModel.from_pretrained(\n",
    "    diffusion_model_id, subfolder='unet', torch_dtype=torch.float16,\n",
    "    revision='fp16', use_auth_token=token)\n",
    "u_net.to(device)\n",
    "\n",
    "# Text Encoder + Tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(text_encoder_model_id)\n",
    "text_encoder = CLIPTextModel.from_pretrained(text_encoder_model_id)\n",
    "text_encoder = text_encoder.to(device)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = DDPMScheduler.from_config(\n",
    "    diffusion_model_id, subfolder='scheduler', torch_dtype=torch.float16,\n",
    "    revision='fp16', use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prompt embeddings\n",
    "def get_text_embeddings(prompt):\n",
    "    # Tokenize text and get embeddings\n",
    "    text_input = tokenizer(\n",
    "        prompt, padding='max_length', max_length=tokenizer.model_max_length,\n",
    "        truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n",
    "\n",
    "    # Do the same for unconditional embeddings\n",
    "    unconditional_input = tokenizer(\n",
    "        [''] * len(prompt), padding='max_length', max_length=tokenizer.model_max_length,\n",
    "        return_tensors='pt')\n",
    "    unconditional_embeddings = text_encoder(unconditional_input.input_ids.to(device))[0]\n",
    "\n",
    "    # Cat for final embeddings\n",
    "    text_embeddings = torch.cat([unconditional_embeddings, text_embeddings])\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_latents(text_embeddings, height=512, width=512,\n",
    "    num_inference_steps=50, guidance_scale=7.5, latents=None):\n",
    "    if latents is None:\n",
    "        latents = torch.randn((text_embeddings.shape[0] // 2, u_net.in_channels, height // 8, width // 8), generator=seed_cpu)\n",
    "        latents = latents.to(device)\n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    latents *= scheduler.init_noise_sigma\n",
    "\n",
    "    for t in tqdm(scheduler.timesteps):\n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = u_net(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # perform guidance\n",
    "        noise_pred_unconditional, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_unconditional + guidance_scale * (noise_pred_text - noise_pred_unconditional)\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img_latents(latents):\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype('uint8')\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4555d70a055476095a439b88575ba89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (float) and bias type (struct c10::Half) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mastronaut in water\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      2\u001b[0m text_embeddings \u001b[39m=\u001b[39m get_text_embeddings(prompt)\n\u001b[1;32m----> 3\u001b[0m latents \u001b[39m=\u001b[39m produce_latents(text_embeddings)\n\u001b[0;32m      4\u001b[0m imgs \u001b[39m=\u001b[39m decode_img_latents(latents)\n\u001b[0;32m      5\u001b[0m imgs[\u001b[39m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn [4], line 17\u001b[0m, in \u001b[0;36mproduce_latents\u001b[1;34m(text_embeddings, height, width, num_inference_steps, guidance_scale, latents)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# predict the noise residual\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 17\u001b[0m     noise_pred \u001b[39m=\u001b[39m u_net(latent_model_input, t, encoder_hidden_states\u001b[39m=\u001b[39;49mtext_embeddings)\u001b[39m.\u001b[39msample\n\u001b[0;32m     19\u001b[0m \u001b[39m# perform guidance\u001b[39;00m\n\u001b[0;32m     20\u001b[0m noise_pred_unconditional, noise_pred_text \u001b[39m=\u001b[39m noise_pred\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mg:\\Benja\\Desktop\\Universidad\\2022\\2do Semestre\\Tópicos Avanzados en Inteligencia Artificial\\AdvancedTopicsIAProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mg:\\Benja\\Desktop\\Universidad\\2022\\2do Semestre\\Tópicos Avanzados en Inteligencia Artificial\\AdvancedTopicsIAProject\\venv\\lib\\site-packages\\diffusers\\models\\unet_2d_condition.py:301\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    298\u001b[0m emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embedding(t_emb)\n\u001b[0;32m    300\u001b[0m \u001b[39m# 2. pre-process\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_in(sample)\n\u001b[0;32m    303\u001b[0m \u001b[39m# 3. down\u001b[39;00m\n\u001b[0;32m    304\u001b[0m down_block_res_samples \u001b[39m=\u001b[39m (sample,)\n",
      "File \u001b[1;32mg:\\Benja\\Desktop\\Universidad\\2022\\2do Semestre\\Tópicos Avanzados en Inteligencia Artificial\\AdvancedTopicsIAProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mg:\\Benja\\Desktop\\Universidad\\2022\\2do Semestre\\Tópicos Avanzados en Inteligencia Artificial\\AdvancedTopicsIAProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mg:\\Benja\\Desktop\\Universidad\\2022\\2do Semestre\\Tópicos Avanzados en Inteligencia Artificial\\AdvancedTopicsIAProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (float) and bias type (struct c10::Half) should be the same"
     ]
    }
   ],
   "source": [
    "prompt = 'astronaut in water'\n",
    "text_embeddings = get_text_embeddings(prompt)\n",
    "latents = produce_latents(text_embeddings)\n",
    "# TODO: fix this\n",
    "#imgs = decode_img_latents(latents)\n",
    "#imgs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cc8010bf3f78e9c10d4febe712c77abe75f8df416c09ebdb6a9b39023fa5c08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
